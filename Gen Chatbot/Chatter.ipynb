{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import string\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file,encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype='float32')\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def pp(strr):\n",
    "    words = strr.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    #stripped = list(filter(None, stripped))\n",
    "    stripped = [word.lower() for word in stripped]\n",
    "    #stripped = [w for w in stripped if not w in stop_words]\n",
    "    #stemmed = [porter.stem(word) for word in stripped]\n",
    "    return(' '.join(stripped))\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j+ 1\n",
    "            \n",
    " \n",
    "    return X_indices\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "  \n",
    "    distance = 0.0\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    \n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/np.multiply(norm_u,norm_v)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    words = sentence.lower().split()\n",
    "    avg = np.zeros((200,))\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = avg/len(words)  \n",
    "    return avg\n",
    "\n",
    "def batao(labl,sent):\n",
    "    mini = 0\n",
    "    for i,n in enumerate(dictt[labl]):\n",
    "            cs = cosine_similarity(n[0],sentence_to_avg(pp(sent),word_to_vec_map))\n",
    "            if(cs>mini):\n",
    "                mini = cs\n",
    "                resp = n[1]\n",
    "    return(resp)\n",
    "def chat2():\n",
    "    print(\"start chit chat! (Enter quit to exit)\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        x_test = np.array([pp(inp)])\n",
    "        X_test_indices = sentences_to_indices(x_test, word_to_index, 24)\n",
    "        pred = model.predict(X_test_indices)\n",
    "        num = np.argmax(pred)\n",
    "        lebl = labllist[num]\n",
    "        print(batao(lebl,inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1125 03:30:32.744243 17520 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1125 03:30:32.776317 17520 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1125 03:30:32.780783 17520 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1125 03:30:33.453666 17520 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1125 03:30:33.471426 17520 deprecation.py:506] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1125 03:30:34.407414 17520 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1125 03:30:36.294779 17520 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1125 03:30:36.786585 17520 deprecation.py:323] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove6b200d/glove.6B.200d.txt')\n",
    "model = load_model('chatter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "xx = []\n",
    "for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            ##stemming to eliminate extra words\n",
    "            ##tokeninze\n",
    "            wrds = nltk.word_tokenize(pattern) ##list of words in patters\n",
    "            words.extend(wrds) ##instead of append\n",
    "            xx.append(pattern)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "            \n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "            \n",
    "labels = sorted(labels)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasa=[]\n",
    "for line in xx:\n",
    "    words = line.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    stripped = [word.lower() for word in stripped]\n",
    "    fasa.append(' '.join(stripped))\n",
    "X_train = [i for i in fasa if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = []\n",
    "for intent in data[\"intents\"]:\n",
    "        for responses in intent[\"responses\"]:\n",
    "            reps.append(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reps)==len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {}\n",
    "for i,n in enumerate(X_train):\n",
    "    if not docs_y[i] in dictt.keys():\n",
    "        dictt[docs_y[i]]=[]\n",
    "    dictt[docs_y[i]].append([sentence_to_avg(pp(n),word_to_vec_map),reps[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Admission' 'Courses' 'Fees' 'General' 'Health' 'Other' 'Refunds'\n",
      " 'goodbye' 'greeting' 'negative' 'positive']\n"
     ]
    }
   ],
   "source": [
    "labllist = np.unique(np.array(docs_y))\n",
    "print(labllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start chit chat! (Enter quit to exit)\n",
      "You: hi\n",
      "Hello, thanks for visiting\n",
      "You: fees rates in quebec\n",
      "Sorry, did I give the wrong answer?\n",
      "You: who sets fees rates in quebec\n",
      "The Quebec Ministry of Education, Leisure and Sports sets the base tuition fee rate as well as the rates of premiums, or forfaitaires, charged to non Quebec residents and International students. Additional information is available on the Ministere de lEducation, Loisirs et Sportswebsite.\n",
      "You: how mch i have to pay\n",
      "Go to your student portal under the MyConcordia Menu choose \"Financial » Students Account\". Your student account will indicate the amount due each term and the deadline date by which your payment must appear on your student account.\n",
      "You: how much i pay\n",
      "Go to your student portal under the MyConcordia Menu choose \"Financial » Students Account\". Your student account will indicate the amount due each term and the deadline date by which your payment must appear on your student account.\n",
      "You: lol\n",
      "See you later, thanks for visiting\n",
      "You: no\n",
      "Is it not what you were looking for?\n",
      "You: stupid\n",
      "Good to see you again\n",
      "You: useless\n",
      "I didn't gete it, Can I help you with something else\n",
      "You: oh i made a spelling mistake there\n",
      "Sorry, did I give the wrong answer?\n",
      "You: yes \n",
      "Good to see you again\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chat2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

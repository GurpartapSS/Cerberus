{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from flask import Flask, render_template, request\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import string\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file,encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype='float32')\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "def pp(strr):\n",
    "    words = strr.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    #stripped = list(filter(None, stripped))\n",
    "    stripped = [word.lower() for word in stripped]\n",
    "    #stripped = [w for w in stripped if not w in stop_words]\n",
    "    #stemmed = [porter.stem(word) for word in stripped]\n",
    "    return(' '.join(stripped))\n",
    "\n",
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        sentence_words = X[i].lower().split()\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j+ 1\n",
    "            \n",
    " \n",
    "    return X_indices\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "  \n",
    "    distance = 0.0\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    \n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/np.multiply(norm_u,norm_v)\n",
    "    \n",
    "    return cosine_similarity\n",
    "\n",
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    words = sentence.lower().split()\n",
    "    avg = np.zeros((200,))\n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = avg/len(words)  \n",
    "    return avg\n",
    "\n",
    "def batao(labl,sent):\n",
    "    mini = 0\n",
    "    for i,n in enumerate(dictt[labl]):\n",
    "            cs = cosine_similarity(n[0],sentence_to_avg(pp(sent),word_to_vec_map))\n",
    "            if(cs>mini):\n",
    "                mini = cs\n",
    "                resp = n[1]\n",
    "    return(resp)\n",
    "def chat2():\n",
    "    print(\"start chit chat! (Enter quit to exit)\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if len(list(j.split(' ')))>24:\n",
    "            raise Exception(\"The length of seentence should be less than 24\")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "        try:\n",
    "            x_test = np.array([pp(inp)])\n",
    "            X_test_indices = sentences_to_indices(x_test, word_to_index, 24)\n",
    "            pred = model.predict(X_test_indices)\n",
    "            num = np.argmax(pred)\n",
    "            lebl = labllist[num]\n",
    "            print(batao(lebl,inp))\n",
    "        except TypeError:\n",
    "            print(\"NULL value\")\n",
    "        except:\n",
    "            print(\"please use \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 16:07:20.474035 16508 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1127 16:07:20.887190 16508 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1127 16:07:20.899471 16508 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1127 16:07:21.887799 16508 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1127 16:07:21.895802 16508 deprecation.py:506] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1127 16:07:25.497738 16508 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1127 16:07:27.071081 16508 deprecation_wrapper.py:119] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1127 16:07:27.223167 16508 deprecation.py:323] From D:\\MiniConda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove6b200d/glove.6B.200d.txt')\n",
    "model = load_model('chatter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "xx = []\n",
    "for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            ##stemming to eliminate extra words\n",
    "            ##tokeninze\n",
    "            wrds = nltk.word_tokenize(pattern) ##list of words in patters\n",
    "            words.extend(wrds) ##instead of append\n",
    "            xx.append(pattern)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "            \n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "            \n",
    "labels = sorted(labels)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasa=[]\n",
    "for line in xx:\n",
    "    words = line.split()\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    stripped = [word.lower() for word in stripped]\n",
    "    fasa.append(' '.join(stripped))\n",
    "X_train = [i for i in fasa if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = []\n",
    "for intent in data[\"intents\"]:\n",
    "        for responses in intent[\"responses\"]:\n",
    "            reps.append(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictt = {}\n",
    "for i,n in enumerate(X_train):\n",
    "    if not docs_y[i] in dictt.keys():\n",
    "        dictt[docs_y[i]]=[]\n",
    "    dictt[docs_y[i]].append([sentence_to_avg(pp(n),word_to_vec_map),reps[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp):\n",
    "    x_test = np.array([pp(inp)])\n",
    "    X_test_indices = sentences_to_indices(x_test, word_to_index, 24)\n",
    "    pred = model.predict(X_test_indices)\n",
    "    num = np.argmax(pred)\n",
    "    lebl = labllist[num]\n",
    "    return (batao(lebl,inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labllist = np.unique(np.array(docs_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    appDict = {\"ans\":\"oh my god!\"}\n",
    "    app_json = json.dumps(appDict)\n",
    "    return appDict\n",
    "\n",
    "@app.route(\"/test_connection/in=<inpp>\")\n",
    "def test_connection(inpp):\n",
    "    #ino=request.args.get(inp)\n",
    "    ans = {\"ans\":predict(inpp)}\n",
    "    return json.dumps(ans)\n",
    "\n",
    "@app.route(\"/indo\")\n",
    "def indo():\n",
    "\treturn render_template(\"WebPage.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1127 16:03:29.101574 16508 _internal.py:122]  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "I1127 16:03:32.052682 23312 _internal.py:122] 127.0.0.1 - - [27/Nov/2019 16:03:32] \"\u001b[37mGET /indo HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start chit chat! (Enter quit to exit)\n",
      "You: i hope this can take more than 24 words and give proper exception, this one should be interesting but lets see\n",
      "Go to your student portal under the MyConcordia Menu choose \"Financial » Students Account\". Your student account will indicate the amount due each term and the deadline date by which your payment must appear on your student account.\n",
      "You: i hope this can take more than 24 words and give proper exception, this one should be interesting but lets see, i guess it was not long enough\n",
      "please use \n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "chat2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
